{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ecuacion Diferencial Lineal de Primer Orden:\n",
    "$$ y'(x)+y(x)=0 \\hspace{1cm} 0 < x < 4 $$ \n",
    "\n",
    "### Condiciones Iniciales:\n",
    "$$ y(0)=1 $$\n",
    "\n",
    "### Solución Analítica:\n",
    "$$ y(x) = e^{x} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODE_1st(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        # Training points and the analytical (exact) solution at this points\n",
    "        x, y_exact = data\n",
    "        # Initial conditions for the PINN\n",
    "        x0=tf.constant([0.0], dtype=tf.float32)\n",
    "        y0_exact=tf.constant([1.0], dtype=tf.float32)\n",
    "        # Calculate the gradients and update weights and bias\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Calculate the gradients dy/dx\n",
    "            with tf.GradientTape() as tape2:\n",
    "              tape2.watch(x0)\n",
    "              tape2.watch(x)\n",
    "              y0_NN = self(x0, training=True)\n",
    "              y_NN  = self(x, training=True)\n",
    "            dy_dx_NN= tape2.gradient(y_NN,x)\n",
    "            #Loss= ODE+ boundary/initial conditions\n",
    "            loss=self.compiled_loss(dy_dx_NN,-y_NN)\\\n",
    "                +self.compiled_loss(y0_NN,y0_exact)\n",
    "        gradients = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "        self.compiled_metrics.update_state(y_exact, y_NN)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 20\n",
    "xmin = 0\n",
    "xmax = 4\n",
    "\n",
    "# Definition of the function domain\n",
    "x_train=np.linspace(xmin,xmax,n_train)\n",
    "\n",
    "# The real solution y(x) for training evaluation\n",
    "y_train=tf.exp(-x_train)\n",
    "\n",
    "# Input and output neurons (from the data)\n",
    "input_neurons  = 1\n",
    "output_neurons = 1\n",
    "\n",
    "# Hiperparameters\n",
    "epochs = 20\n",
    "\n",
    "# Definition of the the model\n",
    "activation='elu'\n",
    "input=Input(shape=(input_neurons,))\n",
    "x=Dense(50, activation=activation)(input)\n",
    "x=Dense(50, activation=activation)(x)\n",
    "x=Dense(50, activation=activation)(x)\n",
    "output = Dense(output_neurons,activation=None)(x)\n",
    "model=ODE_1st(input,output)\n",
    "\n",
    "# Definition of the metrics, optimizer and loss\n",
    "loss= tf.keras.losses.MeanSquaredError()\n",
    "metrics=tf.keras.metrics.MeanSquaredError()\n",
    "optimizer= Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss=loss,\n",
    "          optimizer=optimizer,\n",
    "          metrics=[metrics])\n",
    "model.summary()\n",
    "\n",
    "history=model.fit(x_train, y_train,batch_size=1,epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
