{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ecuacion Diferencial Lineal de Segundo Orden:\n",
    "$$y'' + 3y' - 4y = 0 $$ \n",
    "\n",
    "### Condiciones Iniciales:\n",
    "$$ \\quad y(0) = 1; \\quad y'(0) = -9 $$\n",
    "\n",
    "### Solución Analítica:\n",
    "$$ y(x) = 2e^{−4x} − e^{x} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODE_2nd(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        # Training points and the analytical\n",
    "        # (exact) solution at this points\n",
    "        x, y_exact = data\n",
    "        #Change initial conditions for the PINN\n",
    "        x0=tf.constant([0.0], dtype=tf.float32)\n",
    "        y0_exact=tf.constant([1.0], dtype=tf.float32)\n",
    "        dy_dx0_exact=tf.constant([-9.0], dtype=tf.float32)\n",
    "        # Calculate the gradients and update weights and bias\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x)\n",
    "            tape.watch(y_exact)\n",
    "            tape.watch(x0)\n",
    "            tape.watch(y0_exact)\n",
    "            tape.watch(dy_dx0_exact)\n",
    "            # Calculate the gradients dy2/dx2, dy/dx\n",
    "            with tf.GradientTape() as tape0:\n",
    "                    tape0.watch(x0)\n",
    "                    y0_NN = self(x0,training=False)\n",
    "                    tape0.watch(y0_NN)\n",
    "            dy_dx0_NN = tape0.gradient(y0_NN, x0)\n",
    "            with tf.GradientTape() as tape1:\n",
    "                tape1.watch(x)\n",
    "                with tf.GradientTape() as tape2:\n",
    "                    tape2.watch(x)\n",
    "                    y_NN = self(x,training=False)\n",
    "                    tape2.watch(y_NN)\n",
    "                dy_dx_NN = tape2.gradient(y_NN, x)\n",
    "                tape1.watch(y_NN)\n",
    "                tape1.watch(dy_dx_NN)\n",
    "            d2y_dx2_NN = tape1.gradient(dy_dx_NN, x)\n",
    "            tape.watch(y_NN)\n",
    "            tape.watch(dy_dx_NN)\n",
    "            tape.watch(d2y_dx2_NN)\n",
    "\n",
    "            #Loss= ODE+ boundary/initial conditions\n",
    "            y0_exact=tf.reshape(y0_exact,shape=y_NN[0].shape)\n",
    "            dy_dx0_exact=tf.reshape(dy_dx0_exact,shape=dy_dx0_NN.shape)\n",
    "\n",
    "            loss= self.compiled_loss(y0_NN,y0_exact)\\\n",
    "                  +self.compiled_loss(d2y_dx2_NN,-(3*dy_dx0_NN-4*y_NN))\\\n",
    "                  +self.compiled_loss(dy_dx0_NN,dy_dx0_exact)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "        self.compiled_metrics.update_state(y_exact, y_NN)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 18\n",
    "xmin = 0.0\n",
    "xmax = 8.0\n",
    "\n",
    "# Definition of the function domain\n",
    "x_train=np.linspace(xmin,xmax,n_train)\n",
    "\n",
    "# The solution y(x) for training and validation datasets\n",
    "y_train=np.cos(x_train)\n",
    "\n",
    "# Input and output neurons (from the data)\n",
    "input_neurons  = 1\n",
    "output_neurons = 1\n",
    "\n",
    "# Hiperparameters\n",
    "epochs = 500\n",
    "\n",
    "# Definition of the the model\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=5)\n",
    "activation='tanh'\n",
    "input=Input(shape=(input_neurons,))\n",
    "x=Dense(50, activation=activation,\n",
    "            kernel_initializer=initializer)(input)\n",
    "x=Dense(50, activation=activation,\n",
    "            kernel_initializer=initializer)(x)\n",
    "x=Dense(50, activation=activation,\n",
    "            kernel_initializer=initializer)(x)\n",
    "output = Dense(output_neurons,\n",
    "               activation=None,\n",
    "               kernel_initializer=initializer)(x)\n",
    "model=ODE_2nd(input,output)\n",
    "\n",
    "# Definition of the metrics, optimizer and loss\n",
    "loss= tf.keras.losses.MeanSquaredError()\n",
    "metrics=tf.keras.metrics.MeanSquaredError()\n",
    "optimizer= Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(loss=loss,\n",
    "          optimizer=optimizer,\n",
    "          metrics=[metrics])\n",
    "model.summary()\n",
    "\n",
    "# Just use `fit` as usual\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=1000,\n",
    "                                            restore_best_weights=True)\n",
    "\n",
    "history=model.fit(x_train, y_train,batch_size=1, epochs=epochs,\n",
    "                  callbacks=callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
